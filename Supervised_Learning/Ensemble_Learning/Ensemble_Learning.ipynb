{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning \n",
    "\n",
    "In this notebook, we briefly introduce the ensemble learning algorithms, then apply them on the CVDs dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction \n",
    "\n",
    "**Ensemble methods** are machine learning methods that aggregate the predictions of a group of base learners in order to form a single learning model.\n",
    "\n",
    "In this notebook we will consider four types of ensemble concepts and methods. Namely, \n",
    "1. **Bagging**\n",
    "\n",
    "2. **Random Forests**\n",
    "\n",
    "3. **AdaBoost**\n",
    "\n",
    "4. **gradient boosting**\n",
    "\n",
    "--- \n",
    "\n",
    "## Algorithm and Coding\n",
    "\n",
    "In this part, we will briefly illustrate the algorithm of each ensemble method, then implment them by scikit-learn.\n",
    "\n",
    "### Bagging \n",
    "\n",
    "One way to get a diverse set of classifiers is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called **bagging** (short for bootstrap aggregating). \n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.\n",
    "\n",
    "In order to show the performance of bagging method, we first import the libraries that we need and load the primary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set theme for plotting\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>ATA</th>\n",
       "      <th>NAP</th>\n",
       "      <th>TA</th>\n",
       "      <th>Normal</th>\n",
       "      <th>ST</th>\n",
       "      <th>Flat</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.433140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410909</td>\n",
       "      <td>0.825070</td>\n",
       "      <td>0</td>\n",
       "      <td>1.382928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.478484</td>\n",
       "      <td>0</td>\n",
       "      <td>1.491752</td>\n",
       "      <td>-0.171961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.754157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.751359</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.129513</td>\n",
       "      <td>0.770188</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.525138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.584556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302825</td>\n",
       "      <td>0.139040</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.132156</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051881</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>-0.034755</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.581981</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Sex  RestingBP  Cholesterol  FastingBS     MaxHR  ExerciseAngina  \\\n",
       "0 -1.433140    1   0.410909     0.825070          0  1.382928               0   \n",
       "1 -0.478484    0   1.491752    -0.171961          0  0.754157               0   \n",
       "2 -1.751359    1  -0.129513     0.770188          0 -1.525138               0   \n",
       "3 -0.584556    0   0.302825     0.139040          0 -1.132156               1   \n",
       "4  0.051881    1   0.951331    -0.034755          0 -0.581981               0   \n",
       "\n",
       "   Oldpeak  HeartDisease  ATA  NAP  TA  Normal  ST  Flat  Up  \n",
       "0      0.0             0    1    0   0       1   0     0   1  \n",
       "1      1.0             1    0    1   0       1   0     1   0  \n",
       "2      0.0             0    1    0   0       0   1     0   1  \n",
       "3      1.5             1    0    0   0       1   0     1   0  \n",
       "4      0.0             0    0    1   0       1   0     0   1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/YulinLi98/Sample_Repo/main/heart.csv\")\n",
    "\n",
    "# Data Preprocessing by onehot encoding\n",
    "df.Sex = df.Sex.replace({'M':1, 'F':0})\n",
    "df.ExerciseAngina = df.ExerciseAngina.replace({'Y':1, 'N':0})\n",
    "\n",
    "ChestPainType = pd.get_dummies(df.ChestPainType,drop_first=True)\n",
    "RestingECG = pd.get_dummies(df.RestingECG,drop_first=True)\n",
    "ST_Slope = pd.get_dummies(df.ST_Slope,drop_first=True)\n",
    "df = pd.concat([df,ChestPainType, RestingECG, ST_Slope],axis=1)\n",
    "df.drop(['ChestPainType', 'RestingECG', 'ST_Slope'],axis=1,inplace=True)\n",
    "\n",
    "# Standardize the data\n",
    "df.Age = preprocessing.scale(df.Age)\n",
    "df.RestingBP = preprocessing.scale(df.RestingBP)\n",
    "df.MaxHR = preprocessing.scale(df.MaxHR)\n",
    "df.Cholesterol = preprocessing.scale(df.Cholesterol)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('HeartDisease',axis=1)\n",
    "y = df.HeartDisease\n",
    "\n",
    "# Create a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Scikit-Learn, we import ```DecisionTreeClassifier``` and ```BaggingClassifier```. We instantiate an instance of ```DecisionTreeClassifier``` with max_depth = 1 as the stump classifier, an instance of ```DecisionTreeClassifier``` with max_depth = 3 as the tree classifier, and finally an instance of ```BaggingClassifier``` which trains an ensemble of 500 ```DecisionTreeClassifier``` with max_depth = 1. Then we show the classification reports for these three classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76       112\n",
      "           1       0.85      0.80      0.83       164\n",
      "\n",
      "    accuracy                           0.80       276\n",
      "   macro avg       0.79      0.80      0.80       276\n",
      "weighted avg       0.80      0.80      0.80       276\n",
      " \n",
      "\n",
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       112\n",
      "           1       0.91      0.82      0.86       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.85      0.84       276\n",
      "weighted avg       0.85      0.84      0.84       276\n",
      " \n",
      "\n",
      "Bagging Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76       112\n",
      "           1       0.85      0.80      0.83       164\n",
      "\n",
      "    accuracy                           0.80       276\n",
      "   macro avg       0.79      0.80      0.80       276\n",
      "weighted avg       0.80      0.80      0.80       276\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "stump_clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump_clf.fit(X_train, y_train)\n",
    "stump_y_pred = stump_clf.predict(X_test)\n",
    "print(f\"Stump Classification Report\")\n",
    "print(classification_report(y_test, stump_y_pred), \"\\n\")\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "                            n_estimators = 500,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that the accuracy of the bagging classifier is similar to the stump classifier, and worse than the decision tree classifier. Now let's try to set the *max_depth = 3* in the bagging mehtod and compare it with decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       112\n",
      "           1       0.91      0.82      0.86       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.85      0.84       276\n",
      "weighted avg       0.85      0.84      0.84       276\n",
      " \n",
      "\n",
      "Bagging Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       112\n",
      "           1       0.88      0.86      0.87       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.84      0.84       276\n",
      "weighted avg       0.85      0.84      0.84       276\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "                            n_estimators = 500,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of bagging is the same as the decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forests was developed specifically to address the problem of high-variance in Decision Trees. Like the name suggests, you’re not training a single Decision Tree, you’re training an entire forest! In this case, a forest of Bagged Decision Trees.\n",
    "\n",
    "At a high-level, in pseudo-code, Random Forests algorithm follows these steps:\n",
    "1. Take the original dataset and create N bagged samples of size n, with n smaller than the original dataset.\n",
    "2. Train a Decision Tree with each of the N bagged datasets as input. But, when doing a node split, don’t explore all features in the dataset. Randomly select a smaller number, M features, from all the features in training set. Then pick the best split using impurity measures, like Gini Impurity or Entropy.\n",
    "3. Aggregate the results of the individual decision trees into a single output.\n",
    "4. Average the values for each observation, produced by each tree, if you’re working on a Regression task.\n",
    "5. Do a majority vote across all trees, for each observation, if you’re working on a Classification task.\n",
    "\n",
    "While Forest part of Random Forests refers to training multiple trees, the Random part is present at two different points in the algorithm.\n",
    "\n",
    "There’s the randomness involved in the Bagging process. But then, you also pick a random subset of features to evaluate the node split. This is what guarantees that each tree is different and, therefore, ensures each model produces a slightly different result.\n",
    "\n",
    "In the cell below, we import ```RandomForestClassifier``` from Scikit-Learn, and instantiate an instance of ```RandomForestClassifier``` class with max_depth = 1, then compare its classification result to ```DecisionTreeClassifier``` and ```BaggingClassifier```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       112\n",
      "           1       0.91      0.82      0.86       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.85      0.84       276\n",
      "weighted avg       0.85      0.84      0.84       276\n",
      " \n",
      "\n",
      "Bagging Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76       112\n",
      "           1       0.85      0.80      0.83       164\n",
      "\n",
      "    accuracy                           0.80       276\n",
      "   macro avg       0.79      0.80      0.80       276\n",
      "weighted avg       0.80      0.80      0.80       276\n",
      " \n",
      "\n",
      "Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       112\n",
      "           1       0.86      0.85      0.85       164\n",
      "\n",
      "    accuracy                           0.83       276\n",
      "   macro avg       0.82      0.82      0.82       276\n",
      "weighted avg       0.83      0.83      0.83       276\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "                            n_estimators = 500,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")\n",
    "\n",
    "forest_clf = RandomForestClassifier(max_depth = 1, n_estimators = 500,\n",
    "                                    bootstrap = True,\n",
    "                                    n_jobs = -1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_y_pred = forest_clf.predict(X_test)\n",
    "print(f\"Forest Classification Report\")\n",
    "print(classification_report(y_test, forest_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for random forests is better than bagging, and a bit worse than decision trees. Now, let's change the max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       112\n",
      "           1       0.91      0.82      0.86       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.85      0.84       276\n",
      "weighted avg       0.85      0.84      0.84       276\n",
      " \n",
      "\n",
      "Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       112\n",
      "           1       0.87      0.87      0.87       164\n",
      "\n",
      "    accuracy                           0.84       276\n",
      "   macro avg       0.84      0.84      0.84       276\n",
      "weighted avg       0.84      0.84      0.84       276\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "forest_clf = RandomForestClassifier(max_depth = 3, n_estimators = 500,\n",
    "                                    bootstrap = True,\n",
    "                                    n_jobs = -1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_y_pred = forest_clf.predict(X_test)\n",
    "print(f\"Forest Classification Report\")\n",
    "print(classification_report(y_test, forest_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the max_depth = 3 in Random Forests classfier, the accuracy increases to 0.84, which is similar to the decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance \n",
    "Another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance bylooking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 0.04747167336400108\n",
      "Sex 0.026313650870998725\n",
      "RestingBP 0.009722856673981413\n",
      "Cholesterol 0.04738081146583445\n",
      "FastingBS 0.0137634724994016\n",
      "MaxHR 0.058891161475057324\n",
      "ExerciseAngina 0.12479679115168338\n",
      "Oldpeak 0.11369353851957696\n",
      "ATA 0.040323173087979426\n",
      "NAP 0.014079421628129479\n",
      "TA 0.001560728545987415\n",
      "Normal 0.0015908021139293869\n",
      "ST 0.0008134820099307216\n",
      "Flat 0.1891191141079458\n",
      "Up 0.31047932248556287\n"
     ]
    }
   ],
   "source": [
    "names = X.columns.to_list()\n",
    "for name, score in zip(names, forest_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature importance for 'Up', 'Flat' and 'ExerciseAngina' is higher than other attributes, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost \n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on.\n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "Let’s take a closer look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially\n",
    "set to $\\frac1m$. A first predictor is trained, and its weighted error rate $r_j$ is computed on the training set:\n",
    "\n",
    "$$\n",
    "r_j = \\frac{\\sum_{i=1, \\hat{y}_j^{(i)}\\ne y^{(i)}}^m w^{(i)}}{\\sum_{i=1}^m w^{(i)}},\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_j^{(i)}$ is the $j^{th}$ predictor's prediction for the $i^{th}$ instance. With the weighted error rate, we can compute the predictor's weight $\\alpha_j$: \n",
    "\n",
    "$$\n",
    "\\alpha_j = \\eta \\log \\frac{1-r_j}{r_j},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate hyperparameter. The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.\n",
    "\n",
    "Next, the AdaBoost algorithm updates the instance weights, using the equation below, which boosts the weights of the misclassified instances.\n",
    "\n",
    "Weight update rule: for $i = 1,2,\\cdots, m$\n",
    "\n",
    "$$\n",
    "w^{(i)} = \\begin{cases}\n",
    "w^{(i)}& \\text{if } \\hat{y}_j^{(i)}= y^{(i)}\\\\\n",
    "w^{(i)} \\exp(\\alpha_j) & \\text{if } \\hat{y}_j^{(i)} \\ne y^{(i)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then all the instance weights are normalized. \n",
    "\n",
    "Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes.\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\text{argmax}_k \\sum_{j=1, \\hat{y}_j(x) = k}^N \\alpha_j, \n",
    "$$\n",
    "\n",
    "where $N$ is the number of predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       112\n",
      "           1       0.90      0.85      0.88       164\n",
      "\n",
      "    accuracy                           0.86       276\n",
      "   macro avg       0.85      0.86      0.85       276\n",
      "weighted avg       0.86      0.86      0.86       276\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=42), \n",
    "                             n_estimators = 10,\n",
    "                             algorithm = \"SAMME.R\",\n",
    "                             learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "print(f\"AdaBoost Classification Report\")\n",
    "print(classification_report(y_test, ada_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance for AdaBoost is better than Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "\n",
    "Another very popular boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "We will implement the gradient boosting regressor on the *diabetes* dataset from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data,columns = ['age', 'sex', 'bmi', 'bp', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu'])\n",
    "df['target'] = diabetes.target\n",
    "X = df.drop('target',axis=1)     # the response variables\n",
    "y = df.target                    # the exploratory variable\n",
    "\n",
    "# scale the data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "y_scaled = preprocessing.scale(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for tree_reg is: 0.5874429374145208\n",
      "The R^2 for tree_reg is: 0.355\n",
      "\n",
      "MSE for gb_reg is: 0.4856595296678298\n",
      "The R^2 for gb_reg is: 0.467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.30, random_state=42)\n",
    "\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "print(f\"MSE for tree_reg is: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"The R^2 for tree_reg is: {round(r2_score(y_test, y_pred), 3)}\\n\")\n",
    "\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(max_depth = 2, n_estimators = 150, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "y_pred = gb_reg.predict(X_test)\n",
    "\n",
    "print(f\"MSE for gb_reg is: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"The R^2 for gb_reg is: {round(r2_score(y_test, y_pred), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for gradient boosting regressor is less than decision tree regressor and the $R^2$ for gradient boosting regressor is greater than decision tree regressor, meaning that gradient boosting regressor has a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, we can use early stopping. A simple way to implement this is to use the **staged_predict()** method: it returns an iterator over the predictions made by the ensemble at each stage of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = [mean_squared_error(y_test, y_pred)\n",
    " for y_pred in gb_reg.staged_predict(X_test)]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(range(1,len(errors)+1), errors)\n",
    "plt.plot(range(1,len(errors)+1), errors, color = \"blue\")\n",
    "plt.xlabel('Number of Trees', fontsize = 14)\n",
    "plt.ylabel('Error', fontsize = 14)\n",
    "plt.title('Validation error', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for gbrt_best is: 0.4533168870463858\n",
      "The R^2 for gbrt_best is: 0.502\n"
     ]
    }
   ],
   "source": [
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbrt_best.predict(X_test)\n",
    "\n",
    "print(f\"MSE for gbrt_best is: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"The R^2 for gbrt_best is: {round(r2_score(y_test, y_pred), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ for gradient boosting regressor with diabates data set is greater than 0.50. Compared with k-NN regression and decison tree regression, the performance of gradient boosting regression on diabetes data set is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e3de3013aa64c987b0d0ea220a933b22b0d06ac8866ebafdf417a10062e273"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
